{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "foospong_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnitish18/Multi-Robot_Reinforcement_Learning/blob/main/foospong_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FEDQE3jxhi0J"
      },
      "outputs": [],
      "source": [
        "import pygame, sys, time, random, os\n",
        "from pygame.locals import *\n",
        "import argparse\n",
        "import math\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "# from objectClasses_coupled import *\n",
        "# from train_from_memories_coupled import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class fRect:\n",
        "    \"\"\"Like PyGame's Rect class, but with floating point coordinates\"\"\"\n",
        "\n",
        "    def __init__(self, pos, size):\n",
        "        self.pos = (pos[0], pos[1])\n",
        "        self.size = (size[0], size[1])\n",
        "    def move(self, x, y):\n",
        "        return fRect((self.pos[0]+x, self.pos[1]+y), self.size)\n",
        "\n",
        "    def move_ip(self, x, y, move_factor = 1):\n",
        "        self.pos = (self.pos[0] + x*move_factor, self.pos[1] + y*move_factor)\n",
        "\n",
        "    def get_rect(self):\n",
        "        return Rect(self.pos, self.size)\n",
        "\n",
        "    def copy(self):\n",
        "        return fRect(self.pos, self.size)\n",
        "\n",
        "    def intersect(self, other_frect):\n",
        "        # two rectangles intersect iff both x and y projections intersect\n",
        "        for i in range(2):\n",
        "            if self.pos[i] < other_frect.pos[i]: # projection of self begins to the left\n",
        "                if other_frect.pos[i] >= self.pos[i] + self.size[i]:\n",
        "                    return 0\n",
        "            elif self.pos[i] > other_frect.pos[i]:\n",
        "                if self.pos[i] >= other_frect.pos[i] + other_frect.size[i]:\n",
        "                    return 0\n",
        "        return 1 #self.size > 0 and other_frect.size > 0\n",
        "\n",
        "\n",
        "class Paddle:\n",
        "    def __init__(self, pos, size, speed, max_angle,  facing, timeout, id):\n",
        "        self.frect = fRect((pos[0]-size[0]/2, pos[1]-size[1]/2), size)\n",
        "        self.speed = speed\n",
        "        self.size = size\n",
        "        self.facing = facing\n",
        "        self.max_angle = max_angle\n",
        "        self.timeout = timeout\n",
        "        self.id = id\n",
        "        #self.tf_model = tf_model\n",
        "\n",
        "    def factor_accelerate(self, factor):\n",
        "        self.speed = factor*self.speed\n",
        "\n",
        "    def move(self, i, paddles, balls, table_size, states, withTFmodel, e):\n",
        "        \n",
        "        closest_distance = 10000\n",
        "        closest_ball = None\n",
        "        for ball in balls:\n",
        "            # Checks distance to each ball\n",
        "            if np.linalg.norm(np.asarray(ball.get_center()) - np.asarray(self.frect.pos)) < closest_distance:\n",
        "                closest_distance = np.linalg.norm(np.asarray(ball.get_center()) - np.asarray(self.frect.pos))\n",
        "                closest_ball = ball\n",
        "            \n",
        "        \n",
        "        direction = self.move_getter(withTFmodel, e, states, self.id, self.frect.copy(), closest_ball.frect.copy(), tuple(table_size))\n",
        "        \n",
        "        \n",
        "        if direction == \"up\":\n",
        "            self.frect.move_ip(0, -self.speed)\n",
        "        elif direction == \"down\":\n",
        "            self.frect.move_ip(0, self.speed)\n",
        "\n",
        "#        for j in range(len(paddles)):\n",
        "#            if paddles[j].facing == self.facing and i != j:\n",
        "#\n",
        "#                # bottom of current paddle - top of other paddle (on top of other)\n",
        "#                if ((self.frect.pos[1] + self.frect.size[1]) - (paddles[j].frect.pos[1])) < 0:\n",
        "#                    self.frect.move_ip(0, ((self.frect.pos[1]+self.frect.size[1]) - (paddles[j].frect.pos[1])))\n",
        "#\n",
        "#                # bottom of other paddle - top of current paddle (below other)\n",
        "#                elif ((paddles[j].frect.pos[1] + paddles[j].frect.size[1]) - self.frect.pos[1]) < 0:\n",
        "#                    self.frect.move_ip(0, -((paddles[j].frect.pos[1] + paddles[j].frect.size[1]) - self.frect.pos[1]))\n",
        "#\n",
        "\n",
        "        to_bottom = (self.frect.pos[1]+self.frect.size[1])-table_size[1]\n",
        "        if to_bottom > 0:\n",
        "            self.frect.move_ip(0, -to_bottom)\n",
        "            \n",
        "        to_top = self.frect.pos[1]\n",
        "        if to_top < 0:\n",
        "            self.frect.move_ip(0, -to_top)\n",
        "        \n",
        "        if direction == \"up\":\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "    def get_face_pts(self):\n",
        "        return ((self.frect.pos[0] + self.frect.size[0]*self.facing, self.frect.pos[1]),\n",
        "                (self.frect.pos[0] + self.frect.size[0]*self.facing, self.frect.pos[1] + self.frect.size[1]-1)\n",
        "                )\n",
        "\n",
        "    def get_angle(self, y):\n",
        "        center = self.frect.pos[1]+self.size[1]/2\n",
        "        rel_dist_from_c = ((y-center)/self.size[1])\n",
        "        rel_dist_from_c = min(0.5, rel_dist_from_c)\n",
        "        rel_dist_from_c = max(-0.5, rel_dist_from_c)\n",
        "        sign = 1-2*self.facing\n",
        "\n",
        "        return sign*rel_dist_from_c*self.max_angle*math.pi/180\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Ball:\n",
        "    def __init__(self, table_size, size, paddle_bounce, wall_bounce, dust_error, init_speed_mag):\n",
        "        rand_ang = (.4+.4*random.random())*math.pi*(1-2*(random.random()>.5))+.5*math.pi\n",
        "        speed = (init_speed_mag*math.cos(rand_ang), init_speed_mag*math.sin(rand_ang))\n",
        "        pos = (table_size[0]/2, table_size[1]/2)\n",
        "        self.frect = fRect((pos[0]-size[0]/2, pos[1]-size[1]/2), size)\n",
        "        self.speed = speed\n",
        "        self.size = size\n",
        "        self.paddle_bounce = paddle_bounce\n",
        "        self.wall_bounce = wall_bounce\n",
        "        self.dust_error = dust_error\n",
        "        self.init_speed_mag = init_speed_mag\n",
        "        self.prev_bounce = None\n",
        "        self.lastPaddleIdx = -1\n",
        "\n",
        "    def get_center(self):\n",
        "        return (self.frect.pos[0] + .5*self.frect.size[0], self.frect.pos[1] + .5*self.frect.size[1])\n",
        "\n",
        "\n",
        "    def get_speed_mag(self):\n",
        "        return math.sqrt(self.speed[0]**2+self.speed[1]**2)\n",
        "\n",
        "    def factor_accelerate(self, factor):\n",
        "        self.speed = (factor*self.speed[0], factor*self.speed[1])\n",
        "\n",
        "\n",
        "\n",
        "    def move(self, paddles, table_size, move_factor):\n",
        "        moved = 0\n",
        "        paddled = 0\n",
        "        walls_Rects = [Rect((-100, -100), (table_size[0]+200, 100)),\n",
        "                       Rect((-100, table_size[1]), (table_size[0]+200, 100))]\n",
        "\n",
        "        for wall_rect in walls_Rects:\n",
        "            if self.frect.get_rect().colliderect(wall_rect):\n",
        "                c = 0\n",
        "                \n",
        "                while self.frect.get_rect().colliderect(wall_rect):\n",
        "                    self.frect.move_ip(-.1*self.speed[0], -.1*self.speed[1], move_factor)\n",
        "                    c += 1 # this basically tells us how far the ball has traveled into the wall\n",
        "                r1 = 1+2*(random.random()-.5)*self.dust_error\n",
        "                r2 = 1+2*(random.random()-.5)*self.dust_error\n",
        "\n",
        "                self.speed = (self.wall_bounce*self.speed[0]*r1, -self.wall_bounce*self.speed[1]*r2)\n",
        "                \n",
        "                while c > 0 or self.frect.get_rect().colliderect(wall_rect):\n",
        "                    self.frect.move_ip(.1*self.speed[0], .1*self.speed[1], move_factor)\n",
        "                    c -= 1 # move by roughly the same amount as the ball had traveled into the wall\n",
        "                moved = 1\n",
        "                \n",
        "\n",
        "        for paddle in paddles:\n",
        "            if self.frect.intersect(paddle.frect):\n",
        "                if (paddle.facing == 1 and self.get_center()[0] < paddle.frect.pos[0] + paddle.frect.size[0]/2) or \\\n",
        "                (paddle.facing == 0 and self.get_center()[0] > paddle.frect.pos[0] + paddle.frect.size[0]/2):\n",
        "                    continue\n",
        "                \n",
        "                c = 0\n",
        "                \n",
        "                while self.frect.intersect(paddle.frect) and not self.frect.get_rect().colliderect(walls_Rects[0]) and not self.frect.get_rect().colliderect(walls_Rects[1]):\n",
        "                    self.frect.move_ip(-.1*self.speed[0], -.1*self.speed[1], move_factor)\n",
        "                    \n",
        "                    c += 1\n",
        "                    \n",
        "                theta = paddle.get_angle(self.frect.pos[1]+.5*self.frect.size[1])\n",
        "                \n",
        "\n",
        "                v = self.speed\n",
        "\n",
        "                v = [math.cos(theta)*v[0]-math.sin(theta)*v[1],\n",
        "                             math.sin(theta)*v[0]+math.cos(theta)*v[1]]\n",
        "\n",
        "                v[0] = -v[0]\n",
        "\n",
        "                v = [math.cos(-theta)*v[0]-math.sin(-theta)*v[1],\n",
        "                              math.cos(-theta)*v[1]+math.sin(-theta)*v[0]]\n",
        "\n",
        "\n",
        "                # Bona fide hack: enforce a lower bound on horizontal speed and disallow back reflection\n",
        "#                if  v[0]*(2*paddle.facing-1) < 1: # ball is not traveling (a) away from paddle (b) at a sufficient speed\n",
        "#                    v[1] = (v[1]/abs(v[1]))*math.sqrt(v[0]**2 + v[1]**2 - 1) # transform y velocity so as to maintain the speed\n",
        "#                    v[0] = (2*paddle.facing-1) # note that minimal horiz speed will be lower than we're used to, where it was 0.95 prior to the  increase by 1.2\n",
        "\n",
        "                #a bit hacky, prevent multiple bounces from accelerating\n",
        "                #the ball too much\n",
        "                if not paddle is self.prev_bounce:\n",
        "                    self.speed = (v[0]*self.paddle_bounce, v[1]*self.paddle_bounce)\n",
        "                else:\n",
        "                    self.speed = (v[0], v[1])\n",
        "                self.prev_bounce = paddle\n",
        "                paddled = 1\n",
        "\n",
        "                while c > 0 or self.frect.intersect(paddle.frect):\n",
        "                \n",
        "                    self.frect.move_ip(.1*self.speed[0], .1*self.speed[1], move_factor)\n",
        "                    \n",
        "                    c -= 1\n",
        "                \n",
        "                moved = 1\n",
        "                \n",
        "\n",
        "        if not moved:\n",
        "            self.frect.move_ip(self.speed[0], self.speed[1], move_factor)\n",
        "        \n",
        "        return paddled\n",
        "\n",
        "\n",
        "def directions_from_input(paddle_rect, ball_rect, table_size):\n",
        "    keys = pygame.key.get_pressed()\n",
        "\n",
        "    if keys[pygame.K_UP]:\n",
        "        return \"up\"\n",
        "    elif keys[pygame.K_DOWN]:\n",
        "        return \"down\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def timeout(func, args=(), kwargs={}, timeout_duration=1, default=None):\n",
        "    '''From:\n",
        "    http://code.activestate.com/recipes/473878-timeout-function-using-threading/'''\n",
        "    import threading\n",
        "    class InterruptableThread(threading.Thread):\n",
        "        def __init__(self):\n",
        "            threading.Thread.__init__(self)\n",
        "            self.result = None\n",
        "\n",
        "        def run(self):\n",
        "            try:\n",
        "                self.result = func(*args, **kwargs)\n",
        "            except:\n",
        "                self.result = default\n",
        "\n",
        "    it = InterruptableThread()\n",
        "    it.start()\n",
        "    it.join(timeout_duration)\n",
        "    if it.isAlive():\n",
        "        print(\"TIMEOUT\")\n",
        "        return default\n",
        "    else:\n",
        "        return it.result\n",
        "\n",
        "def render(screen, paddles, balls, score, table_size):\n",
        "    screen.fill(black)\n",
        "\n",
        "    for paddle in paddles:\n",
        "        pygame.draw.rect(screen, white, paddle.frect.get_rect())\n",
        "\n",
        "    for ball in balls:\n",
        "        pygame.draw.circle(screen, white, (int(ball.get_center()[0]), int(ball.get_center()[1])),  int(ball.frect.size[0]/2), 0)\n",
        "\n",
        "\n",
        "    pygame.draw.line(screen, white, [screen.get_width()/2, 0], [screen.get_width()/2, screen.get_height()])\n",
        "\n",
        "    score_font = pygame.font.Font(None, 32)\n",
        "    screen.blit(score_font.render(str(score[0]), True, white), [int(0.4*table_size[0])-8, 0])\n",
        "    screen.blit(score_font.render(str(score[1]), True, white), [int(0.6*table_size[0])-8, 0])\n",
        "\n",
        "    pygame.display.flip()\n",
        "\n",
        "\n",
        "\n",
        "def check_point(score, balls, table_size):\n",
        "    for i in range(len(balls)):\n",
        "        ball = balls[i]\n",
        "        lastPaddleIdxs = []\n",
        "        if ball.frect.pos[0]+ball.size[0]/2 < 0:\n",
        "            score[1] += 1\n",
        "            #tracks which paddle hit the ball last, so that we can attribute the reward to the the right timestep\n",
        "            if ball.prev_bounce is not None and ball.prev_bounce.facing == 0:\n",
        "                lastPaddleIdxs.append(ball.lastPaddleIdx)\n",
        "                \n",
        "            balls[i] = Ball(table_size, ball.size, ball.paddle_bounce, ball.wall_bounce, ball.dust_error, ball.init_speed_mag)\n",
        "            \n",
        "        elif ball.frect.pos[0]+ball.size[0]/2 >= table_size[0]:\n",
        "            balls[i] = Ball(table_size, ball.size, ball.paddle_bounce, ball.wall_bounce, ball.dust_error, ball.init_speed_mag)\n",
        "            score[0] += 1\n",
        "            #return (ball, score)\n",
        "\n",
        "    return (balls, score, lastPaddleIdxs)\n",
        "\n"
      ],
      "metadata": {
        "id": "hOY36H-3By7P"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class foosPong_model(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(foosPong_model, self).__init__()\n",
        "        ###############################################\n",
        "        self.drop = tf.keras.layers.Dropout(0.20)\n",
        "        self.gauss = tf.keras.layers.GaussianNoise(stddev=0.2)\n",
        "        #self.n1 = tf.keras.layers.BatchNormalization()\n",
        "        #self.n2 = tf.keras.layers.BatchNormalization()\n",
        "        \n",
        "        self.d1 = tf.keras.layers.Dense(48, activation='relu')\n",
        "        self.d2 = tf.keras.layers.Dense(48*4, activation='relu')\n",
        "        self.d3 = tf.keras.layers.Dense(48*8, activation='relu')\n",
        "        self.d4 = tf.keras.layers.Dense(48*4, activation='relu')\n",
        "        self.d5 = tf.keras.layers.Dense(48, activation='relu')\n",
        "        \n",
        "        # size 4, so that each teammate has action space of (up, down)\n",
        "        # output here is Q value for each possible action for each teammate, which gets added together in loss function for total max q-value\n",
        "        self.d6 = tf.keras.layers.Dense(4)\n",
        "        \n",
        "        ###############################################\n",
        "        \n",
        "    def call(self, x):\n",
        "        x = self.gauss(x)\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.d3(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.d4(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.d5(x)\n",
        "        return self.d6(x)\n",
        "\n",
        "#def pong_ai(tensor):\n",
        "#\n",
        "#    if paddle_frect.pos[1] + paddle_frect.size[1]/2 < ball_frect.pos[1] + ball_frect.size[1]/2:\n",
        "#        return 1\n",
        "#    else:\n",
        "#        return 0\n",
        "\n",
        "def loss(curr_output, action, reward, target_output):\n",
        "    gamma = 0.95\n",
        "\n",
        "    #curr_action = tf.math.argmax(curr_output, 1)\n",
        "#    curr_output = tf.round(curr_output)\n",
        "#    target_output = tf.round(target_output)\n",
        "    \n",
        "    Q1 = tf.gather(curr_output[:, 0:2], tf.math.argmax(curr_output[:, 0:2], 1), axis=1) + tf.gather(curr_output[:, 2:4], tf.math.argmax(curr_output[:, 2:4], 1), axis=1)\n",
        "    \n",
        "    Q2 = tf.gather(target_output[:, 0:2], tf.math.argmax(target_output[:, 0:2], 1), axis=1) + tf.gather(target_output[:, 2:4], tf.math.argmax(target_output[:, 2:4], 1), axis=1)\n",
        "    \n",
        "    y = gamma*Q2 + reward[:,0]\n",
        "    \n",
        "    loss = tf.keras.losses.MSE(y, Q1)\n",
        "    return loss\n",
        "    \n",
        "def train_nn(memories, curr_model, prev_model):\n",
        "#################################################\n",
        "### Tune these parameters for better training\n",
        "    lr = 0.00000025\n",
        "    #lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4, decay_steps=100, decay_rate=0.5)\n",
        "    epochs = 100\n",
        "    batch_size = 50\n",
        "  #################################################\n",
        "#    lr = 0.00000025\n",
        "#    epochs = 100 # best so far!\n",
        "#    batch_size = 50\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    \n",
        "    \n",
        "    @tf.function\n",
        "    def train(train_data):\n",
        "        for tensor in train_data:\n",
        "            train_step(tensor)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(tensor):\n",
        "        state = tensor[:, :24]\n",
        "        action = tensor[:, 24:26]\n",
        "        reward = tensor[:, 26:28]\n",
        "        next_state = tensor[:, 28:]\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            current_loss = loss(curr_model(state), action, reward, prev_model(next_state))\n",
        "\n",
        "        grad = tape.gradient(current_loss, curr_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grad, curr_model.trainable_variables))\n",
        "        train_loss(current_loss)\n",
        "\n",
        "    train_data = []\n",
        "#    for i in range(memories[0].shape[0]):\n",
        "#        state = memories[0][i,:]\n",
        "#        action = memories[1][i,:]\n",
        "#        reward = memories[2][i,:]\n",
        "#        next_state = memories[3][i,:]\n",
        "#        train_data.append(np.concatenate((state, action, reward, next_state)))\n",
        "\n",
        "    data_size = 10000\n",
        "    #idx = int(np.floor(np.random.random()*(memories[0].shape[0] - data_size)))\n",
        "    for i in range(data_size):\n",
        "        idx = int(np.floor(np.random.random()*(memories[0].shape[0])))\n",
        "            #if idx + i < memories[0].shape[0]:\n",
        "        state = memories[0][idx,:]\n",
        "        action = memories[1][idx,:]\n",
        "        reward = memories[2][idx,:]\n",
        "        next_state = memories[3][idx,:]\n",
        "        train_data.append(np.concatenate((state, action, reward, next_state)))\n",
        "\n",
        "    # could shuffle here. I'm unclear on randomizing each step or maintaining order\n",
        "    train_data_tf = tf.data.Dataset.from_tensor_slices(train_data).shuffle(50000).batch(batch_size)\n",
        "    #train_data_tf = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Reset the metrics at the start of the next epoch\n",
        "        train_loss.reset_states()\n",
        "        train(train_data_tf)\n",
        "        #print(\"works\")\n",
        "        template = '\\nEpoch {}, Loss: {}\\n'\n",
        "        print(template.format(epoch + 1, train_loss.result()))\n",
        "        \n",
        "        #updates target network\n",
        "#        if epoch % 50 == 0:\n",
        "#            prev_model = curr_model\n",
        "    curr_model.save_weights('./trained_weights/foosPong_model_integrated')\n",
        "    return curr_model\n",
        "    \n",
        "#    curr_model.summary()\n",
        "#    #Saves model\n",
        "#    curr_model.save_weights('./trained_weights/foosPong_model_v0')\n"
      ],
      "metadata": {
        "id": "RdR8INA7B-gF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "white = [255, 255, 255]\n",
        "black = [0, 0, 0]\n",
        "clock = pygame.time.Clock()\n",
        "\n",
        "class foosPong_model(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(foosPong_model, self).__init__()\n",
        "        ###############################################\n",
        "        self.drop = tf.keras.layers.Dropout(0.20)\n",
        "        self.gauss = tf.keras.layers.GaussianNoise(stddev=0.2)\n",
        "        #self.n1 = tf.keras.layers.BatchNormalization()\n",
        "        #self.n2 = tf.keras.layers.BatchNormalization()\n",
        "        \n",
        "        self.d1 = tf.keras.layers.Dense(48, activation='relu')\n",
        "        self.d2 = tf.keras.layers.Dense(48*4, activation='relu')\n",
        "        self.d3 = tf.keras.layers.Dense(48*8, activation='relu')\n",
        "        self.d4 = tf.keras.layers.Dense(48*4, activation='relu')\n",
        "        self.d5 = tf.keras.layers.Dense(48, activation='relu')\n",
        "        \n",
        "        # size 4, so that each teammate has action space of (up, down)\n",
        "        # output here is Q value for each possible action for each teammate, which gets added together in loss function for total max q-value\n",
        "        self.d6 = tf.keras.layers.Dense(4)\n",
        "        \n",
        "        ###############################################\n",
        "        \n",
        "    def call(self, x):\n",
        "        x = self.gauss(x)\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.d3(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.d4(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.d5(x)\n",
        "        return self.d6(x)\n"
      ],
      "metadata": {
        "id": "Je3sgg2SAuYf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def game_loop(screen, paddles, balls, table_size, clock_rate, turn_wait_rate, score_to_win, display, e, yesRender=True, withTFmodel=False):\n",
        "    score = [0, 0]\n",
        "    \n",
        "    \n",
        "    \n",
        "    states = [] #state of all paddles and all balls, positions and velocities\n",
        "    actions = [] #actions that each paddle takes\n",
        "    rewards = [] #sum of rewards after each movement\n",
        "    next_states = []\n",
        "\n",
        "    idx = 0\n",
        "    while max(score) < score_to_win:\n",
        "        old_score = score[:]\n",
        "        #print(idx)\n",
        "        \n",
        "        #balls, score = check_point(score, balls, table_size)\n",
        "        \n",
        "        ########### update memories with current states of paddles and balls ############################################################\n",
        "        \n",
        "        curr_states = []\n",
        "        for paddle in paddles:\n",
        "            curr_states.append(paddle.frect.pos[0])\n",
        "            curr_states.append(paddle.frect.pos[1])\n",
        "        for ball in balls:\n",
        "            curr_states.append(ball.get_center()[0])\n",
        "            curr_states.append(ball.get_center()[1])\n",
        "            curr_states.append(ball.speed[0])\n",
        "            curr_states.append(ball.speed[1])\n",
        "        \n",
        "       \n",
        "        # Take actions...and add to memory actions\n",
        "        curr_actions = []\n",
        "        for i in range(len(paddles)):\n",
        "            if paddles[i].facing == 0:\n",
        "                action = paddles[i].move(i, paddles, balls, table_size, curr_states, withTFmodel, e)\n",
        "                curr_actions.append(action)\n",
        "            else:\n",
        "                action = paddles[i].move(i, paddles, balls, table_size, curr_states, False, e)\n",
        "        \n",
        "        \n",
        "        \n",
        "        for ball in balls:\n",
        "            paddled = 0\n",
        "            inv_move_factor = int((ball.speed[0]**2+ball.speed[1]**2)**.5)\n",
        "            if inv_move_factor > 0:\n",
        "                for i in range(inv_move_factor):\n",
        "                    paddled = ball.move(paddles, table_size, 1./inv_move_factor)\n",
        "            else:\n",
        "                paddled = ball.move(paddles, table_size, 1)\n",
        "                \n",
        "            if paddled == 1:\n",
        "                ball.lastPaddleIdx = idx\n",
        "            \n",
        "       \n",
        "        new_states = []\n",
        "        for paddle in paddles:\n",
        "            new_states.append(paddle.frect.pos[0])\n",
        "            new_states.append(paddle.frect.pos[1])\n",
        "        for ball in balls:\n",
        "            new_states.append(ball.get_center()[0])\n",
        "            new_states.append(ball.get_center()[1])\n",
        "            new_states.append(ball.speed[0])\n",
        "            new_states.append(ball.speed[1])\n",
        "        \n",
        "        \n",
        "        # Check if a ball scored and add rewards accordingly, so rewards[i] should correspond to actions taken at actions[i]\n",
        "        balls, score, lastPaddleIdxs = check_point(score, balls, table_size)\n",
        "        \n",
        "        curr_rewards = []\n",
        "        if score != old_score:\n",
        "            if score[0] != old_score[0]:\n",
        "                #-1 for each point opponent scores\n",
        "                curr_rewards.append(-50)\n",
        "                curr_rewards.append(-50)\n",
        "            else:\n",
        "                #+1 each time our team scores\n",
        "                curr_rewards.append(0)\n",
        "                curr_rewards.append(0)\n",
        "                for i in lastPaddleIdxs:\n",
        "                    # adds reward back to the time step that a paddle on our team hit the ball\n",
        "                    if i != -1:\n",
        "                        #print(i)\n",
        "                        #print(idx)\n",
        "                        rewards[i][0] = rewards[i][0] + 100\n",
        "                        rewards[i][1] = rewards[i][1] + 100\n",
        "        else:\n",
        "            # Reward 0 if nothing happens?\n",
        "            curr_rewards.append(0)\n",
        "            curr_rewards.append(0)\n",
        "            \n",
        "        \n",
        "        \n",
        "        if (np.random.random() < 1.0) or score != old_score:\n",
        "            states.append(curr_states)\n",
        "            actions.append(curr_actions)\n",
        "            next_states.append(new_states)\n",
        "            rewards.append(curr_rewards)\n",
        "            idx = idx + 1\n",
        "        \n",
        "\n",
        "\n",
        "################       SCREEN RENDER       ########################\n",
        "\n",
        "        if yesRender:\n",
        "            render(screen, paddles, balls, score, table_size)\n",
        "\n",
        "##########################################################################\n",
        "\n",
        "    for i in range(len(balls)):\n",
        "            balls[i] = Ball(table_size, ball.size, ball.paddle_bounce, ball.wall_bounce, ball.dust_error, ball.init_speed_mag)\n",
        "    \n",
        "    print(score)\n",
        "    print(\"idx\", idx)\n",
        "    print(\"states: \", len(states), \"actions: \", len(actions), \"rewards: \", len(rewards), \"next_states: \", len(next_states))\n",
        "    return states, actions, rewards, next_states\n"
      ],
      "metadata": {
        "id": "9DqXR6k1AyEM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init_game(args):\n",
        "    table_size = (800, 600)\n",
        "    paddle_size = (5, 70)\n",
        "    ball_size = (15, 15)\n",
        "    paddle_speed = 5 #1\n",
        "    max_angle = 45\n",
        "\n",
        "    paddle_bounce = 1.5 #1.2\n",
        "    wall_bounce = 1.00\n",
        "    dust_error = 0.00\n",
        "    init_speed_mag = 2\n",
        "    timeout = 0.0003\n",
        "    clock_rate = 200 #80\n",
        "    turn_wait_rate = 3\n",
        "    score_to_win = 10\n",
        "\n",
        "\n",
        "    screen = pygame.display.set_mode(table_size)\n",
        "    pygame.display.set_caption('PongAIvAI')\n",
        "\n",
        "\n",
        "    paddles = [Paddle((30, table_size[1]/4), paddle_size, paddle_speed, max_angle,  1, timeout, 0), \\\n",
        "               Paddle((300, table_size[1] - table_size[1]/4), paddle_size, paddle_speed, max_angle,  1, timeout, 1), \\\n",
        "               Paddle((table_size[0] - 30, table_size[1]/4), paddle_size, paddle_speed, max_angle,  0, timeout, 0), \\\n",
        "               Paddle((table_size[0] - 300, table_size[1] - table_size[1]/4), paddle_size, paddle_speed, max_angle, 0, timeout, 1)]\n",
        "               \n",
        "    #ball = Ball(table_size, ball_size, paddle_bounce, wall_bounce, dust_error, init_speed_mag)\n",
        "    balls = [Ball(table_size, ball_size, paddle_bounce, wall_bounce, dust_error, init_speed_mag), Ball(table_size, ball_size, paddle_bounce, wall_bounce, dust_error, init_speed_mag), Ball(table_size, ball_size, paddle_bounce, wall_bounce, dust_error, init_speed_mag), Ball(table_size, ball_size, paddle_bounce, wall_bounce, dust_error, init_speed_mag)]\n",
        "    \n",
        "    \n",
        "    \n",
        "    def pong_ai(paddle_frect, ball_frect, table_size):\n",
        "        if paddle_frect.pos[1] + paddle_frect.size[1]/2 < ball_frect.pos[1] + ball_frect.size[1]/2:\n",
        "           return \"down\"\n",
        "        else:\n",
        "           return  \"up\"\n",
        "    \n",
        "    def foosPong_ai(states, id):\n",
        "        output = foosPong(np.asarray(states, dtype='float32').reshape((1,24)))\n",
        "        team_Q_values = tf.reshape(output, [2,2])\n",
        "        action_idx = tf.math.argmax(team_Q_values[id,:]).numpy()\n",
        "        \n",
        "        if action_idx == 0:\n",
        "            return \"down\"\n",
        "        else:\n",
        "            return \"up\"\n",
        "        \n",
        "    def move_getter(withTFmodel, e, states, id, paddle_frect, ball_frect, table_size):\n",
        "        if withTFmodel:\n",
        "            if np.random.random() < e:\n",
        "                return pong_ai(paddle_frect, ball_frect, table_size)\n",
        "            else:\n",
        "                return foosPong_ai(states, id)\n",
        "        else:\n",
        "            return pong_ai(paddle_frect, ball_frect, table_size)\n",
        "    \n",
        "    \n",
        "    # Set move getter functions\n",
        "    paddles[0].move_getter = move_getter\n",
        "    paddles[1].move_getter = move_getter\n",
        "    paddles[2].move_getter = move_getter\n",
        "    paddles[3].move_getter = move_getter\n",
        "        \n",
        "    foosPong = foosPong_model()\n",
        "    # eps = float(args.eps)\n",
        "    eps = 1.0\n",
        "    yesRender = True\n",
        "    withTFmodel = False\n",
        "    if args == 'false': yesRender = False\n",
        "    #if args.withTFmodel == 'true': withTFmodel = True\n",
        "        #foosPong.load_weights('./trained_weights/foosPong_model_v0')  \n",
        "    \n",
        "    episodes = 1000\n",
        "    memory_states = []\n",
        "    memory_actions = []\n",
        "    memory_rewards = []\n",
        "    memory_next_states = []\n",
        "    decay = 0.005\n",
        "    for ep in range(episodes):\n",
        "        print(f\"\\nEpisode: {ep}\")\n",
        "        ep_states, ep_actions, ep_rewards, ep_next_states = game_loop(screen, paddles, balls, table_size, clock_rate, turn_wait_rate, score_to_win, 1, eps-decay*ep, yesRender=yesRender, withTFmodel=withTFmodel)\n",
        "        \n",
        "        memory_states = memory_states + ep_states\n",
        "        memory_actions = memory_actions + ep_actions\n",
        "        memory_rewards = memory_rewards + ep_rewards\n",
        "        memory_next_states = memory_next_states + ep_next_states\n",
        "        print(\"memory_states: \", len(memory_states), \"memory_actions: \", len(memory_actions), \"memory_rewards: \", len(memory_rewards), \"memory_next_states: \", len(memory_next_states), \"\\n\")\n",
        "        \n",
        "        \n",
        "        # after so many steps, take a pause\n",
        "        # foosPong_model = train_nn(memories, foosPong_model)\n",
        "        if len(memory_states) > 50000:\n",
        "            if ep % 25 == 0:\n",
        "                memories = [np.asarray(memory_states, dtype='float32'), np.asarray(memory_actions, dtype='float32'), np.asarray(memory_rewards, dtype='float32'), np.asarray(memory_next_states, dtype='float32')]\n",
        "                \n",
        "                foosPong = train_nn(memories, foosPong, foosPong)\n",
        "            print(\"before\", len(memory_states))\n",
        "            del memory_states[0:len(ep_states)]\n",
        "            del memory_actions[0:len(ep_actions)]\n",
        "            del memory_rewards[0:len(ep_rewards)]\n",
        "            del memory_next_states[0:len(ep_next_states)]\n",
        "            print(\"after\", len(memory_states))\n",
        "            \n",
        "    #    with open(\"memory_states.txt\", \"wb\") as fp:\n",
        "    #        pickle.dump(memory_states, fp)\n",
        "    #    print(\"States dumped...\")\n",
        "    #\n",
        "    #    with open(\"memory_actions.txt\", \"wb\") as fp:\n",
        "    #        pickle.dump(memory_actions, fp)\n",
        "    #    print(\"Actions dumped...\")\n",
        "    #\n",
        "    #    with open(\"memory_rewards.txt\", \"wb\") as fp:\n",
        "    #        pickle.dump(memory_rewards, fp)\n",
        "    #    print(\"Rewards dumped...\")\n",
        "    #\n",
        "    #    with open(\"memory_next_states.txt\", \"wb\") as fp:\n",
        "    #        pickle.dump(memory_next_states, fp)\n",
        "    #    print(\"Next_states dumped...\")\n",
        "    #\n",
        "    #    print(np.asarray(memory_states).shape)\n",
        "    \n",
        "    pygame.quit()\n"
      ],
      "metadata": {
        "id": "pMc2mJaCA4O9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    #parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    #parser.add_argument('--eps')\n",
        "    #parser.add_argument('--yesRender')\n",
        "    #parser.add_argument('--withTFmodel')\n",
        "\n",
        "    args = 'false' \n",
        "    #parser.parse_args()\n",
        "    \n",
        "    pygame.init()\n",
        "    init_game(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7sYGimqBR9G",
        "outputId": "04505555-5757-4ca9-fcf8-818e9fd0fd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Episode: 0\n",
            "[10, 8]\n",
            "idx 1946\n",
            "states:  1946 actions:  1946 rewards:  1946 next_states:  1946\n",
            "memory_states:  1946 memory_actions:  1946 memory_rewards:  1946 memory_next_states:  1946 \n",
            "\n",
            "\n",
            "Episode: 1\n",
            "[10, 9]\n",
            "idx 2431\n",
            "states:  2431 actions:  2431 rewards:  2431 next_states:  2431\n",
            "memory_states:  4377 memory_actions:  4377 memory_rewards:  4377 memory_next_states:  4377 \n",
            "\n",
            "\n",
            "Episode: 2\n",
            "[10, 7]\n",
            "idx 1934\n",
            "states:  1934 actions:  1934 rewards:  1934 next_states:  1934\n",
            "memory_states:  6311 memory_actions:  6311 memory_rewards:  6311 memory_next_states:  6311 \n",
            "\n",
            "\n",
            "Episode: 3\n",
            "[5, 10]\n",
            "idx 1688\n",
            "states:  1688 actions:  1688 rewards:  1688 next_states:  1688\n",
            "memory_states:  7999 memory_actions:  7999 memory_rewards:  7999 memory_next_states:  7999 \n",
            "\n",
            "\n",
            "Episode: 4\n",
            "[2, 10]\n",
            "idx 1158\n",
            "states:  1158 actions:  1158 rewards:  1158 next_states:  1158\n",
            "memory_states:  9157 memory_actions:  9157 memory_rewards:  9157 memory_next_states:  9157 \n",
            "\n",
            "\n",
            "Episode: 5\n",
            "[7, 10]\n",
            "idx 2075\n",
            "states:  2075 actions:  2075 rewards:  2075 next_states:  2075\n",
            "memory_states:  11232 memory_actions:  11232 memory_rewards:  11232 memory_next_states:  11232 \n",
            "\n",
            "\n",
            "Episode: 6\n",
            "[7, 10]\n",
            "idx 2000\n",
            "states:  2000 actions:  2000 rewards:  2000 next_states:  2000\n",
            "memory_states:  13232 memory_actions:  13232 memory_rewards:  13232 memory_next_states:  13232 \n",
            "\n",
            "\n",
            "Episode: 7\n",
            "[10, 5]\n",
            "idx 1885\n",
            "states:  1885 actions:  1885 rewards:  1885 next_states:  1885\n",
            "memory_states:  15117 memory_actions:  15117 memory_rewards:  15117 memory_next_states:  15117 \n",
            "\n",
            "\n",
            "Episode: 8\n",
            "[10, 7]\n",
            "idx 1971\n",
            "states:  1971 actions:  1971 rewards:  1971 next_states:  1971\n",
            "memory_states:  17088 memory_actions:  17088 memory_rewards:  17088 memory_next_states:  17088 \n",
            "\n",
            "\n",
            "Episode: 9\n",
            "[7, 10]\n",
            "idx 1912\n",
            "states:  1912 actions:  1912 rewards:  1912 next_states:  1912\n",
            "memory_states:  19000 memory_actions:  19000 memory_rewards:  19000 memory_next_states:  19000 \n",
            "\n",
            "\n",
            "Episode: 10\n",
            "[3, 10]\n",
            "idx 1414\n",
            "states:  1414 actions:  1414 rewards:  1414 next_states:  1414\n",
            "memory_states:  20414 memory_actions:  20414 memory_rewards:  20414 memory_next_states:  20414 \n",
            "\n",
            "\n",
            "Episode: 11\n",
            "[4, 10]\n",
            "idx 1482\n",
            "states:  1482 actions:  1482 rewards:  1482 next_states:  1482\n",
            "memory_states:  21896 memory_actions:  21896 memory_rewards:  21896 memory_next_states:  21896 \n",
            "\n",
            "\n",
            "Episode: 12\n",
            "[10, 9]\n",
            "idx 2561\n",
            "states:  2561 actions:  2561 rewards:  2561 next_states:  2561\n",
            "memory_states:  24457 memory_actions:  24457 memory_rewards:  24457 memory_next_states:  24457 \n",
            "\n",
            "\n",
            "Episode: 13\n",
            "[10, 6]\n",
            "idx 1763\n",
            "states:  1763 actions:  1763 rewards:  1763 next_states:  1763\n",
            "memory_states:  26220 memory_actions:  26220 memory_rewards:  26220 memory_next_states:  26220 \n",
            "\n",
            "\n",
            "Episode: 14\n",
            "[10, 7]\n",
            "idx 1923\n",
            "states:  1923 actions:  1923 rewards:  1923 next_states:  1923\n",
            "memory_states:  28143 memory_actions:  28143 memory_rewards:  28143 memory_next_states:  28143 \n",
            "\n",
            "\n",
            "Episode: 15\n",
            "[10, 8]\n",
            "idx 3444\n",
            "states:  3444 actions:  3444 rewards:  3444 next_states:  3444\n",
            "memory_states:  31587 memory_actions:  31587 memory_rewards:  31587 memory_next_states:  31587 \n",
            "\n",
            "\n",
            "Episode: 16\n",
            "[10, 9]\n",
            "idx 2298\n",
            "states:  2298 actions:  2298 rewards:  2298 next_states:  2298\n",
            "memory_states:  33885 memory_actions:  33885 memory_rewards:  33885 memory_next_states:  33885 \n",
            "\n",
            "\n",
            "Episode: 17\n",
            "[6, 10]\n",
            "idx 1481\n",
            "states:  1481 actions:  1481 rewards:  1481 next_states:  1481\n",
            "memory_states:  35366 memory_actions:  35366 memory_rewards:  35366 memory_next_states:  35366 \n",
            "\n",
            "\n",
            "Episode: 18\n",
            "[10, 8]\n",
            "idx 2267\n",
            "states:  2267 actions:  2267 rewards:  2267 next_states:  2267\n",
            "memory_states:  37633 memory_actions:  37633 memory_rewards:  37633 memory_next_states:  37633 \n",
            "\n",
            "\n",
            "Episode: 19\n",
            "[9, 10]\n",
            "idx 1900\n",
            "states:  1900 actions:  1900 rewards:  1900 next_states:  1900\n",
            "memory_states:  39533 memory_actions:  39533 memory_rewards:  39533 memory_next_states:  39533 \n",
            "\n",
            "\n",
            "Episode: 20\n",
            "[1, 10]\n",
            "idx 1059\n",
            "states:  1059 actions:  1059 rewards:  1059 next_states:  1059\n",
            "memory_states:  40592 memory_actions:  40592 memory_rewards:  40592 memory_next_states:  40592 \n",
            "\n",
            "\n",
            "Episode: 21\n",
            "[10, 7]\n",
            "idx 2100\n",
            "states:  2100 actions:  2100 rewards:  2100 next_states:  2100\n",
            "memory_states:  42692 memory_actions:  42692 memory_rewards:  42692 memory_next_states:  42692 \n",
            "\n",
            "\n",
            "Episode: 22\n",
            "[3, 10]\n",
            "idx 1260\n",
            "states:  1260 actions:  1260 rewards:  1260 next_states:  1260\n",
            "memory_states:  43952 memory_actions:  43952 memory_rewards:  43952 memory_next_states:  43952 \n",
            "\n",
            "\n",
            "Episode: 23\n",
            "[6, 10]\n",
            "idx 1884\n",
            "states:  1884 actions:  1884 rewards:  1884 next_states:  1884\n",
            "memory_states:  45836 memory_actions:  45836 memory_rewards:  45836 memory_next_states:  45836 \n",
            "\n",
            "\n",
            "Episode: 24\n",
            "[7, 10]\n",
            "idx 2087\n",
            "states:  2087 actions:  2087 rewards:  2087 next_states:  2087\n",
            "memory_states:  47923 memory_actions:  47923 memory_rewards:  47923 memory_next_states:  47923 \n",
            "\n",
            "\n",
            "Episode: 25\n",
            "[6, 10]\n",
            "idx 2077\n",
            "states:  2077 actions:  2077 rewards:  2077 next_states:  2077\n",
            "memory_states:  50000 memory_actions:  50000 memory_rewards:  50000 memory_next_states:  50000 \n",
            "\n",
            "\n",
            "Episode: 26\n",
            "[6, 10]\n",
            "idx 1727\n",
            "states:  1727 actions:  1727 rewards:  1727 next_states:  1727\n",
            "memory_states:  51727 memory_actions:  51727 memory_rewards:  51727 memory_next_states:  51727 \n",
            "\n",
            "before 51727\n",
            "after 50000\n",
            "\n",
            "Episode: 27\n",
            "[10, 8]\n",
            "idx 2151\n",
            "states:  2151 actions:  2151 rewards:  2151 next_states:  2151\n",
            "memory_states:  52151 memory_actions:  52151 memory_rewards:  52151 memory_next_states:  52151 \n",
            "\n",
            "before 52151\n",
            "after 50000\n",
            "\n",
            "Episode: 28\n",
            "[10, 9]\n",
            "idx 2160\n",
            "states:  2160 actions:  2160 rewards:  2160 next_states:  2160\n",
            "memory_states:  52160 memory_actions:  52160 memory_rewards:  52160 memory_next_states:  52160 \n",
            "\n",
            "before 52160\n",
            "after 50000\n",
            "\n",
            "Episode: 29\n",
            "[8, 10]\n",
            "idx 2032\n",
            "states:  2032 actions:  2032 rewards:  2032 next_states:  2032\n",
            "memory_states:  52032 memory_actions:  52032 memory_rewards:  52032 memory_next_states:  52032 \n",
            "\n",
            "before 52032\n",
            "after 50000\n",
            "\n",
            "Episode: 30\n",
            "[8, 10]\n",
            "idx 2515\n",
            "states:  2515 actions:  2515 rewards:  2515 next_states:  2515\n",
            "memory_states:  52515 memory_actions:  52515 memory_rewards:  52515 memory_next_states:  52515 \n",
            "\n",
            "before 52515\n",
            "after 50000\n",
            "\n",
            "Episode: 31\n",
            "[10, 7]\n",
            "idx 1851\n",
            "states:  1851 actions:  1851 rewards:  1851 next_states:  1851\n",
            "memory_states:  51851 memory_actions:  51851 memory_rewards:  51851 memory_next_states:  51851 \n",
            "\n",
            "before 51851\n",
            "after 50000\n",
            "\n",
            "Episode: 32\n",
            "[10, 3]\n",
            "idx 1607\n",
            "states:  1607 actions:  1607 rewards:  1607 next_states:  1607\n",
            "memory_states:  51607 memory_actions:  51607 memory_rewards:  51607 memory_next_states:  51607 \n",
            "\n",
            "before 51607\n",
            "after 50000\n",
            "\n",
            "Episode: 33\n",
            "[11, 8]\n",
            "idx 2564\n",
            "states:  2564 actions:  2564 rewards:  2564 next_states:  2564\n",
            "memory_states:  52564 memory_actions:  52564 memory_rewards:  52564 memory_next_states:  52564 \n",
            "\n",
            "before 52564\n",
            "after 50000\n",
            "\n",
            "Episode: 34\n",
            "[10, 9]\n",
            "idx 2479\n",
            "states:  2479 actions:  2479 rewards:  2479 next_states:  2479\n",
            "memory_states:  52479 memory_actions:  52479 memory_rewards:  52479 memory_next_states:  52479 \n",
            "\n",
            "before 52479\n",
            "after 50000\n",
            "\n",
            "Episode: 35\n",
            "[7, 10]\n",
            "idx 1839\n",
            "states:  1839 actions:  1839 rewards:  1839 next_states:  1839\n",
            "memory_states:  51839 memory_actions:  51839 memory_rewards:  51839 memory_next_states:  51839 \n",
            "\n",
            "before 51839\n",
            "after 50000\n",
            "\n",
            "Episode: 36\n",
            "[10, 8]\n",
            "idx 2139\n",
            "states:  2139 actions:  2139 rewards:  2139 next_states:  2139\n",
            "memory_states:  52139 memory_actions:  52139 memory_rewards:  52139 memory_next_states:  52139 \n",
            "\n",
            "before 52139\n",
            "after 50000\n",
            "\n",
            "Episode: 37\n",
            "[5, 10]\n",
            "idx 1530\n",
            "states:  1530 actions:  1530 rewards:  1530 next_states:  1530\n",
            "memory_states:  51530 memory_actions:  51530 memory_rewards:  51530 memory_next_states:  51530 \n",
            "\n",
            "before 51530\n",
            "after 50000\n",
            "\n",
            "Episode: 38\n",
            "[10, 8]\n",
            "idx 2132\n",
            "states:  2132 actions:  2132 rewards:  2132 next_states:  2132\n",
            "memory_states:  52132 memory_actions:  52132 memory_rewards:  52132 memory_next_states:  52132 \n",
            "\n",
            "before 52132\n",
            "after 50000\n",
            "\n",
            "Episode: 39\n",
            "[7, 10]\n",
            "idx 2177\n",
            "states:  2177 actions:  2177 rewards:  2177 next_states:  2177\n",
            "memory_states:  52177 memory_actions:  52177 memory_rewards:  52177 memory_next_states:  52177 \n",
            "\n",
            "before 52177\n",
            "after 50000\n",
            "\n",
            "Episode: 40\n",
            "[10, 9]\n",
            "idx 1850\n",
            "states:  1850 actions:  1850 rewards:  1850 next_states:  1850\n",
            "memory_states:  51850 memory_actions:  51850 memory_rewards:  51850 memory_next_states:  51850 \n",
            "\n",
            "before 51850\n",
            "after 50000\n",
            "\n",
            "Episode: 41\n",
            "[10, 6]\n",
            "idx 3112\n",
            "states:  3112 actions:  3112 rewards:  3112 next_states:  3112\n",
            "memory_states:  53112 memory_actions:  53112 memory_rewards:  53112 memory_next_states:  53112 \n",
            "\n",
            "before 53112\n",
            "after 50000\n",
            "\n",
            "Episode: 42\n",
            "[10, 8]\n",
            "idx 1952\n",
            "states:  1952 actions:  1952 rewards:  1952 next_states:  1952\n",
            "memory_states:  51952 memory_actions:  51952 memory_rewards:  51952 memory_next_states:  51952 \n",
            "\n",
            "before 51952\n",
            "after 50000\n",
            "\n",
            "Episode: 43\n",
            "[10, 5]\n",
            "idx 1753\n",
            "states:  1753 actions:  1753 rewards:  1753 next_states:  1753\n",
            "memory_states:  51753 memory_actions:  51753 memory_rewards:  51753 memory_next_states:  51753 \n",
            "\n",
            "before 51753\n",
            "after 50000\n",
            "\n",
            "Episode: 44\n",
            "[10, 1]\n",
            "idx 1149\n",
            "states:  1149 actions:  1149 rewards:  1149 next_states:  1149\n",
            "memory_states:  51149 memory_actions:  51149 memory_rewards:  51149 memory_next_states:  51149 \n",
            "\n",
            "before 51149\n",
            "after 50000\n",
            "\n",
            "Episode: 45\n",
            "[5, 10]\n",
            "idx 1505\n",
            "states:  1505 actions:  1505 rewards:  1505 next_states:  1505\n",
            "memory_states:  51505 memory_actions:  51505 memory_rewards:  51505 memory_next_states:  51505 \n",
            "\n",
            "before 51505\n",
            "after 50000\n",
            "\n",
            "Episode: 46\n",
            "[10, 5]\n",
            "idx 1911\n",
            "states:  1911 actions:  1911 rewards:  1911 next_states:  1911\n",
            "memory_states:  51911 memory_actions:  51911 memory_rewards:  51911 memory_next_states:  51911 \n",
            "\n",
            "before 51911\n",
            "after 50000\n",
            "\n",
            "Episode: 47\n",
            "[10, 8]\n",
            "idx 2078\n",
            "states:  2078 actions:  2078 rewards:  2078 next_states:  2078\n",
            "memory_states:  52078 memory_actions:  52078 memory_rewards:  52078 memory_next_states:  52078 \n",
            "\n",
            "before 52078\n",
            "after 50000\n",
            "\n",
            "Episode: 48\n",
            "[6, 10]\n",
            "idx 1706\n",
            "states:  1706 actions:  1706 rewards:  1706 next_states:  1706\n",
            "memory_states:  51706 memory_actions:  51706 memory_rewards:  51706 memory_next_states:  51706 \n",
            "\n",
            "before 51706\n",
            "after 50000\n",
            "\n",
            "Episode: 49\n",
            "[10, 8]\n",
            "idx 1937\n",
            "states:  1937 actions:  1937 rewards:  1937 next_states:  1937\n",
            "memory_states:  51937 memory_actions:  51937 memory_rewards:  51937 memory_next_states:  51937 \n",
            "\n",
            "before 51937\n",
            "after 50000\n",
            "\n",
            "Episode: 50\n",
            "[10, 3]\n",
            "idx 1966\n",
            "states:  1966 actions:  1966 rewards:  1966 next_states:  1966\n",
            "memory_states:  51966 memory_actions:  51966 memory_rewards:  51966 memory_next_states:  51966 \n",
            "\n",
            "\n",
            "Epoch 1, Loss: 12.710676193237305\n",
            "\n",
            "\n",
            "Epoch 2, Loss: 12.634987831115723\n",
            "\n",
            "\n",
            "Epoch 3, Loss: 12.5967435836792\n",
            "\n",
            "\n",
            "Epoch 4, Loss: 12.502564430236816\n",
            "\n",
            "\n",
            "Epoch 5, Loss: 12.44994831085205\n",
            "\n",
            "\n",
            "Epoch 6, Loss: 12.41907024383545\n",
            "\n",
            "\n",
            "Epoch 7, Loss: 12.369180679321289\n",
            "\n",
            "\n",
            "Epoch 8, Loss: 12.309808731079102\n",
            "\n",
            "\n",
            "Epoch 9, Loss: 12.291561126708984\n",
            "\n",
            "\n",
            "Epoch 10, Loss: 12.23923397064209\n",
            "\n",
            "\n",
            "Epoch 11, Loss: 12.221977233886719\n",
            "\n",
            "\n",
            "Epoch 12, Loss: 12.176651954650879\n",
            "\n",
            "\n",
            "Epoch 13, Loss: 12.151180267333984\n",
            "\n",
            "\n",
            "Epoch 14, Loss: 12.122199058532715\n",
            "\n",
            "\n",
            "Epoch 15, Loss: 12.084671974182129\n",
            "\n",
            "\n",
            "Epoch 16, Loss: 12.061592102050781\n",
            "\n",
            "\n",
            "Epoch 17, Loss: 12.03451919555664\n",
            "\n",
            "\n",
            "Epoch 18, Loss: 12.010424613952637\n",
            "\n",
            "\n",
            "Epoch 19, Loss: 11.991514205932617\n",
            "\n",
            "\n",
            "Epoch 20, Loss: 11.990804672241211\n",
            "\n",
            "\n",
            "Epoch 21, Loss: 11.957210540771484\n",
            "\n",
            "\n",
            "Epoch 22, Loss: 11.932354927062988\n",
            "\n",
            "\n",
            "Epoch 23, Loss: 11.925808906555176\n",
            "\n",
            "\n",
            "Epoch 24, Loss: 11.903698921203613\n",
            "\n",
            "\n",
            "Epoch 25, Loss: 11.891016006469727\n",
            "\n",
            "\n",
            "Epoch 26, Loss: 11.879292488098145\n",
            "\n",
            "\n",
            "Epoch 27, Loss: 11.886316299438477\n",
            "\n",
            "\n",
            "Epoch 28, Loss: 11.867481231689453\n",
            "\n",
            "\n",
            "Epoch 29, Loss: 11.852826118469238\n",
            "\n",
            "\n",
            "Epoch 30, Loss: 11.854077339172363\n",
            "\n",
            "\n",
            "Epoch 31, Loss: 11.831134796142578\n",
            "\n",
            "\n",
            "Epoch 32, Loss: 11.823564529418945\n",
            "\n",
            "\n",
            "Epoch 33, Loss: 11.8153715133667\n",
            "\n",
            "\n",
            "Epoch 34, Loss: 11.799798011779785\n",
            "\n",
            "\n",
            "Epoch 35, Loss: 11.804023742675781\n",
            "\n",
            "\n",
            "Epoch 36, Loss: 11.788131713867188\n",
            "\n",
            "\n",
            "Epoch 37, Loss: 11.788064956665039\n",
            "\n",
            "\n",
            "Epoch 38, Loss: 11.768285751342773\n",
            "\n",
            "\n",
            "Epoch 39, Loss: 11.769888877868652\n",
            "\n",
            "\n",
            "Epoch 40, Loss: 11.773977279663086\n",
            "\n",
            "\n",
            "Epoch 41, Loss: 11.737285614013672\n",
            "\n",
            "\n",
            "Epoch 42, Loss: 11.756536483764648\n",
            "\n",
            "\n",
            "Epoch 43, Loss: 11.74675464630127\n",
            "\n",
            "\n",
            "Epoch 44, Loss: 11.734846115112305\n",
            "\n",
            "\n",
            "Epoch 45, Loss: 11.735106468200684\n",
            "\n",
            "\n",
            "Epoch 46, Loss: 11.725363731384277\n",
            "\n",
            "\n",
            "Epoch 47, Loss: 11.730828285217285\n",
            "\n",
            "\n",
            "Epoch 48, Loss: 11.721003532409668\n",
            "\n",
            "\n",
            "Epoch 49, Loss: 11.720458030700684\n",
            "\n",
            "\n",
            "Epoch 50, Loss: 11.719350814819336\n",
            "\n",
            "\n",
            "Epoch 51, Loss: 11.701211929321289\n",
            "\n",
            "\n",
            "Epoch 52, Loss: 11.701815605163574\n",
            "\n",
            "\n",
            "Epoch 53, Loss: 11.688394546508789\n",
            "\n",
            "\n",
            "Epoch 54, Loss: 11.69159984588623\n",
            "\n",
            "\n",
            "Epoch 55, Loss: 11.695837020874023\n",
            "\n",
            "\n",
            "Epoch 56, Loss: 11.684290885925293\n",
            "\n",
            "\n",
            "Epoch 57, Loss: 11.674403190612793\n",
            "\n",
            "\n",
            "Epoch 58, Loss: 11.671568870544434\n",
            "\n",
            "\n",
            "Epoch 59, Loss: 11.679059982299805\n",
            "\n",
            "\n",
            "Epoch 60, Loss: 11.664031982421875\n",
            "\n",
            "\n",
            "Epoch 61, Loss: 11.6669282913208\n",
            "\n",
            "\n",
            "Epoch 62, Loss: 11.664925575256348\n",
            "\n",
            "\n",
            "Epoch 63, Loss: 11.664857864379883\n",
            "\n",
            "\n",
            "Epoch 64, Loss: 11.651942253112793\n",
            "\n",
            "\n",
            "Epoch 65, Loss: 11.665010452270508\n",
            "\n",
            "\n",
            "Epoch 66, Loss: 11.646883010864258\n",
            "\n",
            "\n",
            "Epoch 67, Loss: 11.645808219909668\n",
            "\n",
            "\n",
            "Epoch 68, Loss: 11.637663841247559\n",
            "\n",
            "\n",
            "Epoch 69, Loss: 11.638288497924805\n",
            "\n",
            "\n",
            "Epoch 70, Loss: 11.641939163208008\n",
            "\n",
            "\n",
            "Epoch 71, Loss: 11.64025592803955\n",
            "\n",
            "\n",
            "Epoch 72, Loss: 11.633374214172363\n",
            "\n",
            "\n",
            "Epoch 73, Loss: 11.624853134155273\n",
            "\n",
            "\n",
            "Epoch 74, Loss: 11.621520042419434\n",
            "\n",
            "\n",
            "Epoch 75, Loss: 11.620580673217773\n",
            "\n",
            "\n",
            "Epoch 76, Loss: 11.630960464477539\n",
            "\n",
            "\n",
            "Epoch 77, Loss: 11.620010375976562\n",
            "\n",
            "\n",
            "Epoch 78, Loss: 11.614770889282227\n",
            "\n",
            "\n",
            "Epoch 79, Loss: 11.611503601074219\n",
            "\n",
            "\n",
            "Epoch 80, Loss: 11.606003761291504\n",
            "\n",
            "\n",
            "Epoch 81, Loss: 11.607871055603027\n",
            "\n",
            "\n",
            "Epoch 82, Loss: 11.598509788513184\n",
            "\n",
            "\n",
            "Epoch 83, Loss: 11.607349395751953\n",
            "\n",
            "\n",
            "Epoch 84, Loss: 11.59494686126709\n",
            "\n",
            "\n",
            "Epoch 85, Loss: 11.594042778015137\n",
            "\n",
            "\n",
            "Epoch 86, Loss: 11.599411964416504\n",
            "\n",
            "\n",
            "Epoch 87, Loss: 11.602373123168945\n",
            "\n",
            "\n",
            "Epoch 88, Loss: 11.590597152709961\n",
            "\n",
            "\n",
            "Epoch 89, Loss: 11.58103084564209\n",
            "\n",
            "\n",
            "Epoch 90, Loss: 11.595026016235352\n",
            "\n",
            "\n",
            "Epoch 91, Loss: 11.59155559539795\n",
            "\n",
            "\n",
            "Epoch 92, Loss: 11.582205772399902\n",
            "\n",
            "\n",
            "Epoch 93, Loss: 11.589914321899414\n",
            "\n",
            "\n",
            "Epoch 94, Loss: 11.568432807922363\n",
            "\n",
            "\n",
            "Epoch 95, Loss: 11.583964347839355\n",
            "\n",
            "\n",
            "Epoch 96, Loss: 11.575811386108398\n",
            "\n",
            "\n",
            "Epoch 97, Loss: 11.574080467224121\n",
            "\n",
            "\n",
            "Epoch 98, Loss: 11.579395294189453\n",
            "\n",
            "\n",
            "Epoch 99, Loss: 11.573713302612305\n",
            "\n",
            "\n",
            "Epoch 100, Loss: 11.545888900756836\n",
            "\n",
            "before 51966\n",
            "after 50000\n",
            "\n",
            "Episode: 51\n",
            "[10, 4]\n",
            "idx 1532\n",
            "states:  1532 actions:  1532 rewards:  1532 next_states:  1532\n",
            "memory_states:  51532 memory_actions:  51532 memory_rewards:  51532 memory_next_states:  51532 \n",
            "\n",
            "before 51532\n",
            "after 50000\n",
            "\n",
            "Episode: 52\n",
            "[10, 6]\n",
            "idx 1646\n",
            "states:  1646 actions:  1646 rewards:  1646 next_states:  1646\n",
            "memory_states:  51646 memory_actions:  51646 memory_rewards:  51646 memory_next_states:  51646 \n",
            "\n",
            "before 51646\n",
            "after 50000\n",
            "\n",
            "Episode: 53\n",
            "[2, 10]\n",
            "idx 1247\n",
            "states:  1247 actions:  1247 rewards:  1247 next_states:  1247\n",
            "memory_states:  51247 memory_actions:  51247 memory_rewards:  51247 memory_next_states:  51247 \n",
            "\n",
            "before 51247\n",
            "after 50000\n",
            "\n",
            "Episode: 54\n",
            "[10, 8]\n",
            "idx 2007\n",
            "states:  2007 actions:  2007 rewards:  2007 next_states:  2007\n",
            "memory_states:  52007 memory_actions:  52007 memory_rewards:  52007 memory_next_states:  52007 \n",
            "\n",
            "before 52007\n",
            "after 50000\n",
            "\n",
            "Episode: 55\n",
            "[1, 10]\n",
            "idx 1339\n",
            "states:  1339 actions:  1339 rewards:  1339 next_states:  1339\n",
            "memory_states:  51339 memory_actions:  51339 memory_rewards:  51339 memory_next_states:  51339 \n",
            "\n",
            "before 51339\n",
            "after 50000\n",
            "\n",
            "Episode: 56\n",
            "[6, 10]\n",
            "idx 1921\n",
            "states:  1921 actions:  1921 rewards:  1921 next_states:  1921\n",
            "memory_states:  51921 memory_actions:  51921 memory_rewards:  51921 memory_next_states:  51921 \n",
            "\n",
            "before 51921\n",
            "after 50000\n",
            "\n",
            "Episode: 57\n",
            "[7, 10]\n",
            "idx 1918\n",
            "states:  1918 actions:  1918 rewards:  1918 next_states:  1918\n",
            "memory_states:  51918 memory_actions:  51918 memory_rewards:  51918 memory_next_states:  51918 \n",
            "\n",
            "before 51918\n",
            "after 50000\n",
            "\n",
            "Episode: 58\n",
            "[10, 7]\n",
            "idx 2040\n",
            "states:  2040 actions:  2040 rewards:  2040 next_states:  2040\n",
            "memory_states:  52040 memory_actions:  52040 memory_rewards:  52040 memory_next_states:  52040 \n",
            "\n",
            "before 52040\n",
            "after 50000\n",
            "\n",
            "Episode: 59\n",
            "[10, 7]\n",
            "idx 2291\n",
            "states:  2291 actions:  2291 rewards:  2291 next_states:  2291\n",
            "memory_states:  52291 memory_actions:  52291 memory_rewards:  52291 memory_next_states:  52291 \n",
            "\n",
            "before 52291\n",
            "after 50000\n",
            "\n",
            "Episode: 60\n",
            "[7, 10]\n",
            "idx 1759\n",
            "states:  1759 actions:  1759 rewards:  1759 next_states:  1759\n",
            "memory_states:  51759 memory_actions:  51759 memory_rewards:  51759 memory_next_states:  51759 \n",
            "\n",
            "before 51759\n",
            "after 50000\n",
            "\n",
            "Episode: 61\n",
            "[10, 3]\n",
            "idx 1344\n",
            "states:  1344 actions:  1344 rewards:  1344 next_states:  1344\n",
            "memory_states:  51344 memory_actions:  51344 memory_rewards:  51344 memory_next_states:  51344 \n",
            "\n",
            "before 51344\n",
            "after 50000\n",
            "\n",
            "Episode: 62\n",
            "[6, 10]\n",
            "idx 1982\n",
            "states:  1982 actions:  1982 rewards:  1982 next_states:  1982\n",
            "memory_states:  51982 memory_actions:  51982 memory_rewards:  51982 memory_next_states:  51982 \n",
            "\n",
            "before 51982\n",
            "after 50000\n",
            "\n",
            "Episode: 63\n",
            "[9, 10]\n",
            "idx 1836\n",
            "states:  1836 actions:  1836 rewards:  1836 next_states:  1836\n",
            "memory_states:  51836 memory_actions:  51836 memory_rewards:  51836 memory_next_states:  51836 \n",
            "\n",
            "before 51836\n",
            "after 50000\n",
            "\n",
            "Episode: 64\n",
            "[10, 7]\n",
            "idx 1747\n",
            "states:  1747 actions:  1747 rewards:  1747 next_states:  1747\n",
            "memory_states:  51747 memory_actions:  51747 memory_rewards:  51747 memory_next_states:  51747 \n",
            "\n",
            "before 51747\n",
            "after 50000\n",
            "\n",
            "Episode: 65\n",
            "[10, 3]\n",
            "idx 1499\n",
            "states:  1499 actions:  1499 rewards:  1499 next_states:  1499\n",
            "memory_states:  51499 memory_actions:  51499 memory_rewards:  51499 memory_next_states:  51499 \n",
            "\n",
            "before 51499\n",
            "after 50000\n",
            "\n",
            "Episode: 66\n",
            "[9, 10]\n",
            "idx 2159\n",
            "states:  2159 actions:  2159 rewards:  2159 next_states:  2159\n",
            "memory_states:  52159 memory_actions:  52159 memory_rewards:  52159 memory_next_states:  52159 \n",
            "\n",
            "before 52159\n",
            "after 50000\n",
            "\n",
            "Episode: 67\n",
            "[10, 5]\n",
            "idx 1809\n",
            "states:  1809 actions:  1809 rewards:  1809 next_states:  1809\n",
            "memory_states:  51809 memory_actions:  51809 memory_rewards:  51809 memory_next_states:  51809 \n",
            "\n",
            "before 51809\n",
            "after 50000\n",
            "\n",
            "Episode: 68\n",
            "[5, 10]\n",
            "idx 1584\n",
            "states:  1584 actions:  1584 rewards:  1584 next_states:  1584\n",
            "memory_states:  51584 memory_actions:  51584 memory_rewards:  51584 memory_next_states:  51584 \n",
            "\n",
            "before 51584\n",
            "after 50000\n",
            "\n",
            "Episode: 69\n",
            "[10, 5]\n",
            "idx 2231\n",
            "states:  2231 actions:  2231 rewards:  2231 next_states:  2231\n",
            "memory_states:  52231 memory_actions:  52231 memory_rewards:  52231 memory_next_states:  52231 \n",
            "\n",
            "before 52231\n",
            "after 50000\n",
            "\n",
            "Episode: 70\n",
            "[6, 10]\n",
            "idx 1863\n",
            "states:  1863 actions:  1863 rewards:  1863 next_states:  1863\n",
            "memory_states:  51863 memory_actions:  51863 memory_rewards:  51863 memory_next_states:  51863 \n",
            "\n",
            "before 51863\n",
            "after 50000\n",
            "\n",
            "Episode: 71\n",
            "[7, 10]\n",
            "idx 2258\n",
            "states:  2258 actions:  2258 rewards:  2258 next_states:  2258\n",
            "memory_states:  52258 memory_actions:  52258 memory_rewards:  52258 memory_next_states:  52258 \n",
            "\n",
            "before 52258\n",
            "after 50000\n",
            "\n",
            "Episode: 72\n",
            "[5, 10]\n",
            "idx 1502\n",
            "states:  1502 actions:  1502 rewards:  1502 next_states:  1502\n",
            "memory_states:  51502 memory_actions:  51502 memory_rewards:  51502 memory_next_states:  51502 \n",
            "\n",
            "before 51502\n",
            "after 50000\n",
            "\n",
            "Episode: 73\n",
            "[10, 8]\n",
            "idx 1896\n",
            "states:  1896 actions:  1896 rewards:  1896 next_states:  1896\n",
            "memory_states:  51896 memory_actions:  51896 memory_rewards:  51896 memory_next_states:  51896 \n",
            "\n",
            "before 51896\n",
            "after 50000\n",
            "\n",
            "Episode: 74\n",
            "[10, 7]\n",
            "idx 2040\n",
            "states:  2040 actions:  2040 rewards:  2040 next_states:  2040\n",
            "memory_states:  52040 memory_actions:  52040 memory_rewards:  52040 memory_next_states:  52040 \n",
            "\n",
            "before 52040\n",
            "after 50000\n",
            "\n",
            "Episode: 75\n",
            "[9, 10]\n",
            "idx 2365\n",
            "states:  2365 actions:  2365 rewards:  2365 next_states:  2365\n",
            "memory_states:  52365 memory_actions:  52365 memory_rewards:  52365 memory_next_states:  52365 \n",
            "\n",
            "\n",
            "Epoch 1, Loss: 13.611842155456543\n",
            "\n",
            "\n",
            "Epoch 2, Loss: 13.609382629394531\n",
            "\n",
            "\n",
            "Epoch 3, Loss: 13.60729694366455\n",
            "\n",
            "\n",
            "Epoch 4, Loss: 13.600889205932617\n",
            "\n",
            "\n",
            "Epoch 5, Loss: 13.587554931640625\n",
            "\n",
            "\n",
            "Epoch 6, Loss: 13.596715927124023\n",
            "\n",
            "\n",
            "Epoch 7, Loss: 13.578142166137695\n",
            "\n",
            "\n",
            "Epoch 8, Loss: 13.578615188598633\n",
            "\n",
            "\n",
            "Epoch 9, Loss: 13.576494216918945\n",
            "\n",
            "\n",
            "Epoch 10, Loss: 13.580024719238281\n",
            "\n",
            "\n",
            "Epoch 11, Loss: 13.570867538452148\n",
            "\n",
            "\n",
            "Epoch 12, Loss: 13.566575050354004\n",
            "\n",
            "\n",
            "Epoch 13, Loss: 13.569196701049805\n",
            "\n",
            "\n",
            "Epoch 14, Loss: 13.569899559020996\n",
            "\n",
            "\n",
            "Epoch 15, Loss: 13.554417610168457\n",
            "\n",
            "\n",
            "Epoch 16, Loss: 13.55821418762207\n",
            "\n",
            "\n",
            "Epoch 17, Loss: 13.54308795928955\n",
            "\n",
            "\n",
            "Epoch 18, Loss: 13.553360939025879\n",
            "\n",
            "\n",
            "Epoch 19, Loss: 13.543472290039062\n",
            "\n",
            "\n",
            "Epoch 20, Loss: 13.54148006439209\n",
            "\n",
            "\n",
            "Epoch 21, Loss: 13.542621612548828\n",
            "\n",
            "\n",
            "Epoch 22, Loss: 13.552009582519531\n",
            "\n",
            "\n",
            "Epoch 23, Loss: 13.539776802062988\n",
            "\n",
            "\n",
            "Epoch 24, Loss: 13.544525146484375\n",
            "\n",
            "\n",
            "Epoch 25, Loss: 13.527070045471191\n",
            "\n",
            "\n",
            "Epoch 26, Loss: 13.545208930969238\n",
            "\n",
            "\n",
            "Epoch 27, Loss: 13.53537368774414\n",
            "\n",
            "\n",
            "Epoch 28, Loss: 13.533575057983398\n",
            "\n",
            "\n",
            "Epoch 29, Loss: 13.538084030151367\n",
            "\n",
            "\n",
            "Epoch 30, Loss: 13.518698692321777\n",
            "\n",
            "\n",
            "Epoch 31, Loss: 13.5390043258667\n",
            "\n",
            "\n",
            "Epoch 32, Loss: 13.525837898254395\n",
            "\n",
            "\n",
            "Epoch 33, Loss: 13.52284049987793\n",
            "\n",
            "\n",
            "Epoch 34, Loss: 13.51927661895752\n",
            "\n",
            "\n",
            "Epoch 35, Loss: 13.516515731811523\n",
            "\n",
            "\n",
            "Epoch 36, Loss: 13.519309043884277\n",
            "\n",
            "\n",
            "Epoch 37, Loss: 13.51388168334961\n",
            "\n",
            "\n",
            "Epoch 38, Loss: 13.512068748474121\n",
            "\n",
            "\n",
            "Epoch 39, Loss: 13.499613761901855\n",
            "\n",
            "\n",
            "Epoch 40, Loss: 13.51093864440918\n",
            "\n",
            "\n",
            "Epoch 41, Loss: 13.511344909667969\n",
            "\n",
            "\n",
            "Epoch 42, Loss: 13.523224830627441\n",
            "\n",
            "\n",
            "Epoch 43, Loss: 13.50989055633545\n",
            "\n",
            "\n",
            "Epoch 44, Loss: 13.512866973876953\n",
            "\n",
            "\n",
            "Epoch 45, Loss: 13.501169204711914\n",
            "\n",
            "\n",
            "Epoch 46, Loss: 13.503656387329102\n",
            "\n"
          ]
        }
      ]
    }
  ]
}